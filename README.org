#+TITLE: BERTax training utilities
#+OPTIONS: ^:nil
This repository contains utilities for pre-training and fine-tuning [[https://github.com/f-kretschmer/bertax][BERTax]] models, as
well as various utility functions and scripts used in the development of BERTax.

* Modules
** =models=
Contains:
- declaration of "conventional" neural network architectures like
  /Convolutional Neural Networks/ (CNN), /Recurrent Neural Networks/
  (RNN) and /Temporal Convolutional Network/ (TCN):
  [[file:models/model.py]], to be used in conjunction with
  [[file:utils/test_model.py]]
- scripts for pre-training and fine-tuning the [[https://github.com/google-research/bert][BERT (Bidirectional
  Encoder Representations from Transformers)]] neural network
  architecture, divided into versions for both /coding/ and
  /non-coding/ (genomic; used in [[https://github.com/f-kretschmer/bertax][BERTax]]) DNA sequences
  | mode         | sequences | script                              |
  |--------------+-----------+-------------------------------------|
  | pre-training | coding    | [[file:models/bert_pretrain.py]]    |
  | pre-training | genomic   | [[file:models/bert_nc.py]]          |
  | fine-tuning  | coding    | [[file:models/bert_finetune.py]]    |
  | fine-tuning  | genomic   | [[file:models/bert_nc_finetune.py]] |

** =preprocessing=
Utilities for sequence input, encoding and model input generation:
- [[file:preprocessing/process_inputs.py]] :: functions for reading in
  and encoding sequences
- [[file:preprocessing/generate_data.py]] :: Generator classes for
  storing and retrieving training / validation / testing data
** =utils=
Contains the CLI program for training/testing conventional neural network
architectures (CNN/RNN/TCN), [[file:utils/test_model.py]], as well as
a CLI program to apply/test fine-tuned BERT models, for example with
=fasta= sequences, [[file:utils/test_bert.py]]
* Usage
** Training models
*** Data preparation
Two modes exist for preparing raw DNA sequences for training
**** Individual sequence fastas (=gene=)
Each sequence is contained in a fasta file, additionally, a =json=
file containg all file-names and associated classes can speed up
preprocessing tremendously. A fixed directory structure is
requiered:
#+begin_example
[class_1]/
  [sequence_1.fa]
  [seuqence_2.fa]
  ...
  [sequence_n.fa]
[class_2]/
  ...
.../
[class_n]/
  ...
  [sequence_l.fa]
{files.json}
#+end_example

The =json=-files cotains a list of two lists with equal size, the
first list contains filepaths to the fasta files and the second list
the associated classes:
#+begin_src json
[["class_1/sequence1.fa", "class_1/sequence2.fa", ..., "class_n/sequence_l.fa"],
 ["class_1", "class_1", ..., "class_n"]]
#+end_src
**** single sequences =json= (=genome=, BERTax)
This mode requires a fixed directory structure with one =json= file
per class, consisting of a simple list of sequences:
#+begin_example
[class_1]/
  [class_1]_fragments.json
[class_2]/
  [class_2]_fragments.json
.../
[class_n]/
  [class_n]_fragments.json
#+end_example

Example fragments file:
#+begin_src json
["ACGTACGTACGATCGA", "TACACTTTTTA", ..., "ATACTATCTATCTA"]
#+end_src
*** Training and testing models
All Scripts described here are implemented as CLIs; detailed usage
information can be optained via the =--help= flag.

For RNN, CNN and TCN models, the script [[file:utils/test_model.py]] is used:
#+begin_src shell
  python utils/test_model.py tcn --nr_seqs 10_000 --summary \
	 --root_fa_dir sequences --file_names_cache sequences/files.json
#+end_src

To pre-train BERT (gene) models (Script [[file:models/bert_pretrain.py]]):
#+begin_src shell
  python -m models.bert_pretrain bert_gene_C2 --epochs 10 --batch_size 32 --seq_len 502 \
	 --head_num 5 --embed_dim 250 --feed_forward_dim 1024 --dropout_rate 0.05 \
	 --root_fa_dir sequences --from_cache sequences/files.json
#+end_src

To fine-tune BERT (gene) models (Script [[file:models/bert_finetune.py]])
#+begin_src shell
  python -m models.bert_finetune bert_gene_C2_trained.h5 --epochs 4 \
	 --root_fa_dir sequences --from_cache sequences/files.json
#+end_src

The scripts [[file:models/bert_nc.py]] and [[file:models/bert_nc_finetune.py]] are used
analogously for genomic (BERTax) models, with the exception of input specification:

#+begin_src shell
  python -m models.bert_nc single_sequences_json_folder/
#+end_src

#+begin_src shell
  python -m models.bert_nc_finetune bert_nc_trained.h5 single_sequences_json_folder/
#+end_src

**** Benchmarking 
If the user needs a predefined training and test set, for example for benchmarking different approaches:

#+begin_src shell
  python -m preprocessing.make_dataset single_sequences_json_folder/ out_folder/ --unbalanced
#+end_src
This creates a the files test.tsv, train.tsv, classes.pkl which can be used by bert_nc_finetune

#+begin_src shell
  python -m models.bert_nc_finetune bert_nc_trained.h5 make_dataset_out_folder/ --unbalanced --use_defined_train_test_set
#+end_src

If fasta files are necessary, e.g., for competing methods, you can parse the train.tsv and test.tsv via
#+begin_src shell
  python -m preprocessing.dataset2fasta make_dataset_out_folder/
#+end_src


** Using BERT models

A script is available to predict sequences in using a BERT model.
For example, sequences contained in a fasta file can be predicted:

#+begin_src fasta
> class_1
ACGTAGCTA
> class_2
ACATATATTATATTTT
#+end_src

#+begin_src shell
python -m utils.test_bert finetuned_bert.h5 --fasta sequences.fa
#+end_src

For this script =--help= provides further usage information.

For many more options the finetuned model can also be used in [[https://github.com/f-kretschmer/bertax][BERTax]] with the parameter
=--custom_model_file=.

* Dependencies
- tensorflow >= 2
- keras
- numpy
- tqdm
- scikit-learn
- keras-bert
- keras-tcn

** Visualization
- torch
- transformers
- bio
