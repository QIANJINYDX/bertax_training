#+TITLE: Classification of DNA Sequences
#+OPTIONS: ^:nil
Library for classifying DNA Sequences using various artificial neural
network architectures.

The implemented classification criterion is taxonomy, specifically
at the level of /superkingdoms/.

* Modules
** =models=
Contains:
- declaration of conventional neural network architectures like
  /Convolutional Neural Networks/ (CNN), /Recurrent Neural Networks/
  (RNN) and /Temporal Convolutional Network/ (TCN):
  [[file:models/model.py]], to be used in conjunction with
  [[file:utils/test_model.py]]
- scripts for pre-training and fine-tuning the [[https://github.com/google-research/bert][BERT (Bidirectional
  Encoder Representations from Transformers)]] neural network
  architecture, divided into versions for both /coding/ and
  /non-coding/ (genomic) DNA sequences
  | mode         | sequences | script                              |
  |--------------+-----------+-------------------------------------|
  | pre-training | coding    | [[file:models/bert_pretrain.py]]    |
  | pre-training | genomic   | [[file:models/bert_nc.py]]          |
  | fine-tuning  | coding    | [[file:models/bert_finetune.py]]    |
  | fine-tuning  | genomic   | [[file:models/bert_nc_finetune.py]] |

** =preprocessing=
Utilities for sequence input, encoding and model input generation:
- [[file:preprocessing/process_inputs.py]] :: functions for reading in
  and encoding sequences
- [[file:preprocessing/generate_data.py]] :: Generator classes for
  storing and retrieving training / validation / testing data
** =utils=
Contains the CLI program for training/testing conventional neural network
architectures (CNN/RNN/TCN), [[file:utils/test_model.py]], as well as
a CLI program to apply/test fine-tuned BERT models, for example with
=fasta= sequences, [[file:utils/test_bert.py]]
* Usage
** Training models
*** Data preparation
Two modes exist for preparing raw DNA sequences for training
**** Individual sequence fastas (=gene=)
Each sequence is contained in a fasta file, additionally, a =json=
file containg all file-names and associated classes can speed up
preprocessing tremendously. A fixed directory structure is
requiered:
#+begin_example
[class_1]/
  [sequence_1.fa]
  [seuqence_2.fa]
  ...
  [sequence_n.fa]
[class_2]/
  ...
.../
[class_n]/
  ...
  [sequence_l.fa]
{files.json}
#+end_example

The =json=-files cotains a list of two lists with equal size, the
first list contains filepaths to the fasta files and the second list
the associated classes:
#+begin_src json
[["class_1/sequence1.fa", "class_1/sequence2.fa", ..., "class_n/sequence_l.fa"],
 ["class_1", "class_1", ..., "class_n"]]
#+end_src
**** single sequence =json=
This mode requires a fixed directory structure with one =json= file
per class, consisting of a simple list of sequences:
#+begin_example
[class_1]/
  [class_1]_fragments.json
[class_2]/
  [class_2]_fragments.json
.../
[class_n]/
  [class_n]_fragments.json
#+end_example

Example fragments file:
#+begin_src json
["ACGTACGTACGATCGA", "TACACTTTTTA", ..., "ATACTATCTATCTA"]
#+end_src
*** Training and testing models
All Scripts described here are implemented as CLIs; detailed usage
information can be optained via the =--help= flag.

For RNN, CNN and TCN models, the script [[file:utils/test_model.py]] is used:
#+begin_src shell
  python utils/test_model.py tcn --nr_seqs 10_000 --summary \
	 --root_fa_dir sequences --file_names_cache sequences/files.json
#+end_src

To pre-train BERT (gene) models (Script [[file:models/bert_pretrain.py]]):
#+begin_src shell
  python -m models.bert_pretrain bert_gene_C2 --epochs 10 --batch_size 32 --seq_len 502 \
	 --head_num 5 --embed_dim 250 --feed_forward_dim 1024 --dropout_rate 0.05 \
	 --root_fa_dir sequences --from_cache sequences/files.json
#+end_src

To fine-tune BERT (genomic) models (Script [[file:models/bert_finetune.py]])
#+begin_src shell
  python -m models.bert_finetune bert_gene_C2_trained.h5 --epochs 4 \
	 --root_fa_dir sequences --from_cache sequences/files.json
#+end_src

The scripts [[file:models/bert_nc.py]] and [[file:models/bert_nc_finetune.py]] are used
analogously, with the exception of sequence specification:

#+begin_src shell
  python -m models.bert_nc single_sequence_json_folder/
#+end_src

#+begin_src shell
  python -m models.bert_nc_finetune bert_nc_trained.h5 single_sequence_json_folder/
#+end_src
** Using BERT models

A script is available to predict sequences in using a BERT model.
For example, sequences contained in a fasta file can be predicted:

#+begin_src fasta
> class_1
ACGTAGCTA
> class_2
ACATATATTATATTTT
#+end_src

#+begin_src shell
python -m utils.test_bert finetuned_bert.h5 --fasta sequences.fa
#+end_src

The best-performing fine-tuned BERT models ready to use are contained in the
directory =resources/=.

For this script =--help= provides further usage information.
** Visualization

To visualize a trained model, use [[file:utils/bertviz_js.py]]:
#+begin_src sh
python -m utils.bertviz_js test/random_archaean_fragment.fa resources/bert_nc_C2_final.h5 -a 441 -n 30 --mode head
#+end_src

Note, that using long sequences (> 150 tokens) puts a heavy burden on memory and performance.

* Dependencies
- tensorflow >= 2
- keras
- numpy
- tqdm
- scikit-learn
- keras-bert
- keras-tcn

** Visualization
- torch
- transformers
- bio
